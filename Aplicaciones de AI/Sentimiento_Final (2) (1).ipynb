{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import *\n",
    "import pandas as pd\n",
    "import re #sirve para concatenar texto\n",
    "import nltk #procesamiento de palabras\n",
    "from nltk.corpus import stopwords\n",
    "from pysentimiento.preprocessing import preprocess_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tiempo</th>\n",
       "      <th>Perfil</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832421505778618368</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Josefina Vázquez Mota debe informar qué hizo c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833130565087985664</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Ya ni la burla perdonan: bajaron 2 centavos la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833501308548177921</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Martín Moreno no votará por mí. Comprendo. Es ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>833895239211364357</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>En Chicago dije que iremos a Nueva York (ONU) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>834183763923841024</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Entérate y apoya con tu firma la denuncia cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>834565985583525888</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Qué república ni qué ocho cuartos, es la monar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>835297483521564672</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Salieron a defender a Yunes Linares, Calderón,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>835903628388990976</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>En 2010, la trasnacional Odebrecht entregó sob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>836417496420925440</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Calderón dice donará su sueldo como expresiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>836745914022682624</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>¡Las pensiones de los expresidentes de México ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tiempo         Perfil  \\\n",
       "0  832421505778618368  lopezobrador_   \n",
       "1  833130565087985664  lopezobrador_   \n",
       "2  833501308548177921  lopezobrador_   \n",
       "3  833895239211364357  lopezobrador_   \n",
       "4  834183763923841024  lopezobrador_   \n",
       "5  834565985583525888  lopezobrador_   \n",
       "6  835297483521564672  lopezobrador_   \n",
       "7  835903628388990976  lopezobrador_   \n",
       "8  836417496420925440  lopezobrador_   \n",
       "9  836745914022682624  lopezobrador_   \n",
       "\n",
       "                                                text  \n",
       "0  Josefina Vázquez Mota debe informar qué hizo c...  \n",
       "1  Ya ni la burla perdonan: bajaron 2 centavos la...  \n",
       "2  Martín Moreno no votará por mí. Comprendo. Es ...  \n",
       "3  En Chicago dije que iremos a Nueva York (ONU) ...  \n",
       "4  Entérate y apoya con tu firma la denuncia cont...  \n",
       "5  Qué república ni qué ocho cuartos, es la monar...  \n",
       "6  Salieron a defender a Yunes Linares, Calderón,...  \n",
       "7  En 2010, la trasnacional Odebrecht entregó sob...  \n",
       "8  Calderón dice donará su sueldo como expresiden...  \n",
       "9  ¡Las pensiones de los expresidentes de México ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Carlos/Documents/Carlos/Cenfotec/Clase 05 -Procesamiento del Lenguaje 2/tuits_bayes.csv\", encoding =\"latin-1\")\n",
    "df.head(10) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.rename(columns={'Comentario':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = ['con']\n",
    "def cleaner(word): \n",
    "    word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', str(word), flags=re.MULTILINE)\n",
    "    word = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \"\", str(word))\n",
    "    word = re.sub(r'ee.uu', 'eeuu', str(word))\n",
    "    word = re.sub(r'\\#\\.', '', str(word))\n",
    "    word = re.sub(r'\\n', '', str(word))\n",
    "    word = re.sub(r',', '', str(word))\n",
    "    word = re.sub(r'\\-', ' ', str(word))\n",
    "    word = re.sub(r'\\.{3}', ' ', str(word))\n",
    "    word = re.sub(r'a{2,}', 'a', str(word))\n",
    "    word = re.sub(r'é{2,}', 'é', str(word))\n",
    "    word = re.sub(r'i{2,}', 'i', str(word))\n",
    "    word = re.sub(r'ja{2,}', 'ja', str(word)) \n",
    "    word = re.sub(r'á', 'a', str(word))\n",
    "    word = re.sub(r'é', 'e', str(word))\n",
    "    word = re.sub(r'í', 'i', str(word))\n",
    "    word = re.sub(r'ó', 'o', str(word))\n",
    "    word = re.sub(r'ú', 'u', str(word))  \n",
    "    word = re.sub('[^a-zA-Z]', ' ', str(word))\n",
    "    word = \" \".join([i for i in word.split() if i not in stopwords])\n",
    "    word = preprocess_tweet(str(word))\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1349"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('spanish'))\n",
    "additional_stopwords=set(black_list)\n",
    "stopwords = stop.union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text'] = df['text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tiempo</th>\n",
       "      <th>Perfil</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832421505778618368</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Josefina Vazquez Mota debe informar hizo mil m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833130565087985664</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Ya burla perdonan bajaron centavos gasolina En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833501308548177921</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Martin Moreno votara Comprendo Es mal escritor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>833895239211364357</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>En Chicago dije iremos Nueva York ONU Washingt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>834183763923841024</td>\n",
       "      <td>lopezobrador_</td>\n",
       "      <td>Enterate apoya firma denuncia ordenes Donald T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>983475079744049152</td>\n",
       "      <td>DafneFCh</td>\n",
       "      <td>Hay cosas puede hacer Y simplemente rinde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>983475080507338753</td>\n",
       "      <td>09osuna</td>\n",
       "      <td>Ser feliz implica aceptar limitaciones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>983475080566136832</td>\n",
       "      <td>louder_thancat</td>\n",
       "      <td>Nada parece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>983475082570772485</td>\n",
       "      <td>lilyindescai</td>\n",
       "      <td>Cuando empiezas acordar cosas hiciste ebriedad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>983475083053281281</td>\n",
       "      <td>perecerda</td>\n",
       "      <td>Fulvio Martins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1349 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tiempo          Perfil  \\\n",
       "0     832421505778618368   lopezobrador_   \n",
       "1     833130565087985664   lopezobrador_   \n",
       "2     833501308548177921   lopezobrador_   \n",
       "3     833895239211364357   lopezobrador_   \n",
       "4     834183763923841024   lopezobrador_   \n",
       "...                  ...             ...   \n",
       "1344  983475079744049152        DafneFCh   \n",
       "1345  983475080507338753         09osuna   \n",
       "1346  983475080566136832  louder_thancat   \n",
       "1347  983475082570772485    lilyindescai   \n",
       "1348  983475083053281281       perecerda   \n",
       "\n",
       "                                                   text  \n",
       "0     Josefina Vazquez Mota debe informar hizo mil m...  \n",
       "1     Ya burla perdonan bajaron centavos gasolina En...  \n",
       "2     Martin Moreno votara Comprendo Es mal escritor...  \n",
       "3     En Chicago dije iremos Nueva York ONU Washingt...  \n",
       "4     Enterate apoya firma denuncia ordenes Donald T...  \n",
       "...                                                 ...  \n",
       "1344          Hay cosas puede hacer Y simplemente rinde  \n",
       "1345             Ser feliz implica aceptar limitaciones  \n",
       "1346                                        Nada parece  \n",
       "1347     Cuando empiezas acordar cosas hiciste ebriedad  \n",
       "1348                                     Fulvio Martins  \n",
       "\n",
       "[1349 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cbd8c814a4446a85730fc11662cf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlos\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Carlos\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f637850aed4f3db9ec6cf85a000441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b379ee40f8584747a05491cd9642eb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d9292b6f95493985b4c50383eb94ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/925 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695dd060a3ce419cb5a41f90b83aa733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/435M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6910b7df01d1474aa5229b161f4816f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154028ff315d4197b9bfd08132c84503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4f597bea0e46e4b52841d7c0bba720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277ffbc719734aa6ae433803c96fb37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b26fb174bb4977a9a3c856e56a1778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3caa099aa2243aca0a6d721ad5d04a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\vocab.txt\n",
      "loading file bpe.codes from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\bpe.codes\n",
      "loading file added_tokens.json from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "loading configuration file config.json from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2ff6f700c44be3ac7a0688dd9835af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Carlos/.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-emotion-analysis\\snapshots\\64046df9cc41eab40e1ecde7d2b7fb42b971be5b\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "emotion_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    res1 = analyzer.predict(row)\n",
    "    res2 = emotion_analyzer.predict(row)\n",
    "    return res1.output,res2.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['senti_puntuacion','Emotion'] = df['text'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel (r'C:/Users/Carlos/Documents/Carlos/Cenfotec/Clase 05 -Procesamiento del Lenguaje 2/sentimientoene.xlsx', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "# Replaces user handles and URLs by special tokens\n",
    "preprocess_tweet(\"@perezjotaeme debería cambiar esto http://bit.ly/sarasa\") # \"@usuario debería cambiar esto url\"\n",
    "\n",
    "# Shortens repeated characters\n",
    "preprocess_tweet(\"no entiendo naaaaaaaadaaaaaaaa\", shorten=2) # \"no entiendo naadaa\"\n",
    "\n",
    "# Normalizes laughters\n",
    "preprocess_tweet(\"jajajajaajjajaajajaja no lo puedo creer ajajaj\") # \"jaja no lo puedo creer jaja\"\n",
    "\n",
    "# Handles hashtags\n",
    "preprocess_tweet(\"esto es #UnaGenialidad\")\n",
    "# \"esto es una genialidad\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
